import math
import re
import warnings

from einops import repeat
import torch
import torch.nn as nn

from utils import print_on_rank_zero


def show_state_dict(state_dict):
	for name, value in state_dict.items():
		print(name)


def replace_state_dict(state_dict):
	for old_key in list(state_dict.keys()):
		if old_key.startswith('model'):
			new_key = old_key[6:] # skip 'model.'
			if 'in_proj' in new_key:
				new_key = new_key.replace('in_proj_', 'qkv.') #in_proj_weight -> qkv.weight
			elif 'out_proj' in new_key:
				new_key = new_key.replace('out_proj', 'proj')
			state_dict[new_key] = state_dict.pop(old_key)
		else: # cls_head
			new_key = old_key[9:]
			state_dict[new_key] = state_dict.pop(old_key)


def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # type: (Tensor, float, float, float, float) -> Tensor
    r"""Fills the input Tensor with values drawn from a truncated
    normal distribution. The values are effectively drawn from the
    normal distribution :math:`\mathcal{N}(\text{mean}, \text{std}^2)`
    with values outside :math:`[a, b]` redrawn until they are within
    the bounds. The method used for generating the random values works
    best when :math:`a \leq \text{mean} \leq b`.
    Args:
        tensor: an n-dimensional `torch.Tensor`
        mean: the mean of the normal distribution
        std: the standard deviation of the normal distribution
        a: the minimum cutoff value
        b: the maximum cutoff value
    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.trunc_normal_(w)
    """
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)

    
@torch.no_grad()   
def constant_init_(tensor, constant_value=0):
	nn.init.constant_(tensor, constant_value)
	

@torch.no_grad()
def kaiming_init_(tensor,
                  a=0,
                  mode='fan_out',
                  nonlinearity='relu',
                  distribution='normal'):
    assert distribution in ['uniform', 'normal']
    if distribution == 'uniform':
    	nn.init.kaiming_uniform_(
    		tensor, a=a, mode=mode, nonlinearity=nonlinearity)
    else:
    	nn.init.kaiming_normal_(
    		tensor, a=a, mode=mode, nonlinearity=nonlinearity)


@torch.no_grad()	
def init_from_vit_pretrain_(module,
						    pretrained,
						    conv_type, 
						    attention_type, 
						    copy_strategy,
						    extend_strategy='temporal_avg',
						    tube_size=2, 
						    num_time_transformer_layers=4):

	if isinstance(pretrained, str):
		if torch.cuda.is_available():
			state_dict = torch.load(pretrained)
		else:
			state_dict = torch.load(pretrained, map_location=torch.device('cpu'))
		
		if 'state_dict' in state_dict:
			state_dict = state_dict['state_dict']
		
		old_state_dict_keys = list(state_dict.keys())
		for old_key in old_state_dict_keys:
			# extend the Conv2d params to Conv3d
			if conv_type == 'Conv3d':
				if 'patch_embed.projection.weight' in old_key:
					weight = state_dict[old_key]
					new_weight = repeat(weight, 'd c h w -> d c t h w', t=tube_size)
					if extend_strategy == 'temporal_avg':
						new_weight = new_weight / tube_size
					elif extend_strategy == 'center_frame':
						new_weight.zero_()
						new_weight[:,:,tube_size//2,:,:] = weight
					state_dict[old_key] = new_weight
					continue
					
			# modify the key names of norm layers
			if attention_type == 'fact_encoder':
				new_key = old_key.replace('transformer_layers.layers',
										  'transformer_layers.0.layers')
			else:
				new_key = old_key
			
			if 'in_proj' in new_key:
				new_key = new_key.replace('in_proj_', 'qkv.') #in_proj_weight -> qkv.weight
			elif 'out_proj' in new_key:
				new_key = new_key.replace('out_proj', 'proj')

			if 'norms' in new_key:
				new_key = new_key.replace('norms.0', 'attentions.0.norm')
				new_key = new_key.replace('norms.1', 'ffns.0.norm')

			state_dict[new_key] = state_dict.pop(old_key)

		old_state_dict_keys = list(state_dict.keys())
		for old_key in old_state_dict_keys:
			# copy the parameters of space attention to time attention
			if attention_type == 'divided_space_time':
				if 'attentions.0' in old_key:
					new_key = old_key.replace('attentions.0',
											  'attentions.1')
					if copy_strategy == 'repeat':
						state_dict[new_key] = state_dict[old_key].clone()
					elif copy_strategy == 'set_zero':
						state_dict[new_key] = state_dict[old_key].clone().zero_()
			# copy the part of parameters of space attention to time attention
			elif attention_type == 'fact_encoder':
				pattern = re.compile(r'(?<=layers.)\d+')
				matchObj = pattern.findall(old_key)
				if len(matchObj) > 1 and int(matchObj[1]) < num_time_transformer_layers:
					new_key = old_key.replace('transformer_layers.0.layers', 
											  'transformer_layers.1.layers')
					if copy_strategy == 'repeat':
						state_dict[new_key] = state_dict[old_key].clone()
					elif copy_strategy == 'set_zero':
						state_dict[new_key] = state_dict[old_key].clone().zero_()

		missing_keys,unexpected_keys = module.load_state_dict(state_dict, strict=False)
		#print(f'missing_keys:{missing_keys}\n unexpected_keys:{unexpected_keys}')
		print_on_rank_zero(f'missing_keys:{missing_keys}\n '
						   f'unexpected_keys:{unexpected_keys}')


@torch.no_grad()
def init_from_mae_pretrain_(module,
							pretrained,
							conv_type, 
							attention_type, 
							copy_strategy,
							extend_strategy='temporal_avg',
							tube_size=2, 
							num_time_transformer_layers=4):

	if isinstance(pretrained, str):
		if torch.cuda.is_available():
			state_dict = torch.load(pretrained)
		else:
			state_dict = torch.load(pretrained, map_location=torch.device('cpu'))
		
		if 'model' in state_dict:
			state_dict = state_dict['model']
		
		# adjust to our module
		old_state_dict_keys = list(state_dict.keys())
		for old_key in old_state_dict_keys:
			if 'decoder' in old_key:
				state_dict.pop(old_key)
				continue
			
			# extend the Conv2d params to Conv3d
			if 'encoder.patch_embed.proj' in old_key:
				new_key = old_key.replace('encoder.patch_embed.proj',
										  'patch_embed.projection')
				if conv_type == 'Conv3d' and 'weight' in old_key:
					weight = state_dict[old_key]
					new_weight = repeat(weight, 'd c h w -> d c t h w', t=tube_size)
					if extend_strategy == 'temporal_avg':
						new_weight = new_weight / tube_size
					elif extend_strategy == 'center_frame':
						new_weight.zero_()
						new_weight[:,:,tube_size//2,:,:] = weight
					state_dict.pop(old_key)
					state_dict[new_key] = new_weight
				else:
					state_dict[new_key] = state_dict.pop(old_key)
				continue

			# modify the key names of norm layers
			if attention_type == 'fact_encoder':
				new_key = old_key.replace('encoder.blocks',
										  'transformer_layers.0.layers')
			else:
				new_key = old_key.replace('encoder.blocks',
										  'transformer_layers.layers')

			if 'norm' in new_key:
				new_key = new_key.replace('norm1', 'attentions.0.norm')
				new_key = new_key.replace('norm2', 'ffns.0.norm')
			elif 'attn' in new_key:
				#new_key = new_key.replace('attn.qkv.weight',
				#						  'attentions.0.attn.in_proj_weight')
				#new_key = new_key.replace('attn.proj',
				#						  'attentions.0.attn.out_proj')
				if 'q_bias' in new_key:
					pattern = re.compile(r'(?<=blocks.)\d+')
					matchObj = pattern.findall(old_key)
					block_id = int(matchObj[0])
					q_bias = state_dict[f'encoder.blocks.{block_id}.attn.q_bias']
					v_bias = state_dict[f'encoder.blocks.{block_id}.attn.v_bias']
					weight = torch.cat((q_bias, 
										torch.zeros_like(q_bias, requires_grad=False),
										v_bias))
					new_key = new_key.replace('attn.q_bias',
											  #'attentions.0.attn.in_proj_bias')
											  'attentions.0.attn.qkv.bias')
					state_dict.pop(f'encoder.blocks.{block_id}.attn.q_bias')
					state_dict.pop(f'encoder.blocks.{block_id}.attn.v_bias')
					state_dict[new_key] = weight
					continue
				elif 'v_bias' in new_key:
					continue
			elif 'mlp' in new_key:
				new_key = new_key.replace('mlp.fc1', 'ffns.0.layers.0.0')
				new_key = new_key.replace('mlp.fc2', 'ffns.0.layers.1')
			
			if 'encoder.norm' in old_key:
				new_key = old_key.replace('encoder.norm',
										  'norm')

			state_dict[new_key] = state_dict.pop(old_key)

		# copy to new layer
		old_state_dict_keys = list(state_dict.keys())
		for old_key in old_state_dict_keys:
			# copy the parameters of space attention to time attention
			if attention_type == 'divided_space_time':
				if 'attentions.0' in old_key:
					new_key = old_key.replace('attentions.0',
											  'attentions.1')
					if copy_strategy == 'repeat':
						state_dict[new_key] = state_dict[old_key].clone()
					elif copy_strategy == 'set_zero':
						state_dict[new_key] = state_dict[old_key].clone().zero_()
			# copy the part of parameters of space attention to time attention
			elif attention_type == 'fact_encoder':
				pattern = re.compile(r'(?<=layers.)\d+')
				matchObj = pattern.findall(old_key)
				if len(matchObj) > 1 and int(matchObj[1]) < num_time_transformer_layers:
					new_key = old_key.replace('transformer_layers.0.layers', 
											  'transformer_layers.1.layers')
					if copy_strategy == 'repeat':
						state_dict[new_key] = state_dict[old_key].clone()
					elif copy_strategy == 'set_zero':
						state_dict[new_key] = state_dict[old_key].clone().zero_()

		missing_keys,unexpected_keys = module.load_state_dict(state_dict, strict=False)
		#print(f'missing_keys:{missing_keys}\n unexpected_keys:{unexpected_keys}')
		print_on_rank_zero(f'missing_keys:{missing_keys}\n '
						   f'unexpected_keys:{unexpected_keys}')
						   

def init_from_kinetics_pretrain_(module, pretrain_pth):
	if torch.cuda.is_available():
		state_dict = torch.load(pretrain_pth)
	else:
		state_dict = torch.load(pretrain_pth, map_location=torch.device('cpu'))
	if 'state_dict' in state_dict:
		state_dict = state_dict['state_dict']
	
	replace_state_dict(state_dict)
	msg = module.load_state_dict(state_dict, strict=False)
	print_on_rank_zero(msg)